# ONNX Runtime GPUæ¨è«–ã‚¬ã‚¤ãƒ‰

RTX 5070 Tiç­‰ã®æ–°ã—ã„GPUã§é«˜é€Ÿæ¨è«–ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

---

## ğŸ¯ æ¦‚è¦

ONNX Runtimeã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€PyTorchãŒæœªå¯¾å¿œã®GPUï¼ˆRTX 5070 Tiç­‰ï¼‰ã§ã‚‚GPUæ¨è«–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

**ãƒ¡ãƒªãƒƒãƒˆ:**
- âœ… æœ€æ–°GPUå¯¾å¿œï¼ˆRTX 5070 Ti, RTX 50ã‚·ãƒªãƒ¼ã‚ºç­‰ï¼‰
- âœ… æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- âœ… DirectMLå¯¾å¿œï¼ˆWindowsï¼‰
- âœ… CPUæ¨è«–ã‚‚é«˜é€ŸåŒ–

---

## ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```powershell
pip install onnx onnxruntime
```

### GPUå¯¾å¿œï¼ˆWindows - DirectMLï¼‰

```powershell
pip install onnxruntime-directml
```

**æ³¨æ„:** `onnxruntime`ã¨`onnxruntime-directml`ã¯åŒæ™‚ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ãã¾ã›ã‚“ã€‚DirectMLã‚’ä½¿ã†å ´åˆã¯`onnxruntime`ã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```powershell
pip uninstall onnxruntime
pip install onnxruntime-directml
```

### GPUå¯¾å¿œï¼ˆLinux/Windows - CUDAï¼‰

```powershell
pip install onnxruntime-gpu
```

---

## ğŸš€ ä½¿ã„æ–¹

### ã‚¹ãƒ†ãƒƒãƒ—1: PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«å¤‰æ›

```powershell
cd segmentation
python export_onnx.py --model models/best_unet.pth
```

**å‡ºåŠ›:** `models/best_unet.onnx`

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³:**
```powershell
python export_onnx.py \
  --model models/best_unet.pth \
  --output models/custom_name.onnx \
  --height 512 \
  --width 512 \
  --opset 14
```

---

### ã‚¹ãƒ†ãƒƒãƒ—2: ONNX Runtimeã§æ¨è«–

#### CPUæ¨è«–

```powershell
python inference_onnx.py \
  --images <ç”»åƒãƒ•ã‚©ãƒ«ãƒ€> \
  --output <å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€> \
  --device cpu
```

#### GPUæ¨è«–ï¼ˆDirectML - Windowsï¼‰

```powershell
python inference_onnx.py \
  --images <ç”»åƒãƒ•ã‚©ãƒ«ãƒ€> \
  --output <å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€> \
  --device directml
```

ã¾ãŸã¯

```powershell
python inference_onnx.py \
  --images <ç”»åƒãƒ•ã‚©ãƒ«ãƒ€> \
  --output <å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€> \
  --directml
```

#### GPUæ¨è«–ï¼ˆCUDA - Linux/Windowsï¼‰

```powershell
python inference_onnx.py \
  --images <ç”»åƒãƒ•ã‚©ãƒ«ãƒ€> \
  --output <å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€> \
  --device cuda
```

---

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

### äºˆæƒ³ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

| ç’°å¢ƒ | ç›¸å¯¾é€Ÿåº¦ | å‚™è€ƒ |
|-----|---------|------|
| PyTorch CPU | 1x | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ |
| ONNX Runtime CPU | 1.5-2x | æœ€é©åŒ–ã«ã‚ˆã‚Šé«˜é€ŸåŒ– |
| ONNX Runtime DirectML (RTX 5070 Ti) | 10-30x | GPUåŠ é€Ÿ |
| PyTorch CUDA (å¯¾å¿œGPU) | 10-50x | å‚è€ƒå€¤ |

**æ³¨æ„:** å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ç”»åƒã‚µã‚¤ã‚ºã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã€GPUæ€§èƒ½ã«ä¾å­˜ã—ã¾ã™ã€‚

---

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚¨ãƒ©ãƒ¼: onnxruntimeãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“

```powershell
pip install onnxruntime
```

GPUæ¨è«–ã®å ´åˆ:
```powershell
# Windows
pip install onnxruntime-directml

# Linux/Windows (CUDA)
pip install onnxruntime-gpu
```

### ã‚¨ãƒ©ãƒ¼: ONNXãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“

```powershell
cd segmentation
python export_onnx.py --model models/best_unet.pth
```

### DirectMLãŒå‹•ä½œã—ãªã„

**ç¢ºèªäº‹é …:**
1. Windows 10/11ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
2. æœ€æ–°ã®GPUãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
3. `onnxruntime`ã¨`onnxruntime-directml`ãŒç«¶åˆã—ã¦ã„ãªã„ã‹ç¢ºèª

```powershell
# ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip uninstall onnxruntime onnxruntime-directml -y
pip install onnxruntime-directml
```

### CUDAãŒå‹•ä½œã—ãªã„

**ç¢ºèªäº‹é …:**
1. NVIDIA GPUãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
2. CUDA Toolkit 11.xä»¥ä¸ŠãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
3. CUDAã«å¯¾å¿œã—ãŸ`onnxruntime-gpu`ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨

---

## ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ–¹æ³•

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ

```python
import time
import numpy as np

# PyTorchæ¨è«–
from inference import inference

start = time.time()
inference(images_dir='test_images', output_dir='output_pytorch', create_video=False)
pytorch_time = time.time() - start

# ONNX Runtimeæ¨è«–
from inference_onnx import inference_onnx

start = time.time()
inference_onnx(images_dir='test_images', output_dir='output_onnx', 
               device='directml', create_video=False)
onnx_time = time.time() - start

print(f"PyTorch: {pytorch_time:.2f}ç§’")
print(f"ONNX Runtime: {onnx_time:.2f}ç§’")
print(f"é«˜é€ŸåŒ–: {pytorch_time / onnx_time:.2f}x")
```

---

## ğŸ”„ PyTorchã¨ã®åˆ‡ã‚Šæ›¿ãˆ

### æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã®é¸æŠ

#### PyTorchï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

```powershell
python run_inference_analysis.py --images <dir> --output <dir>
```

#### ONNX Runtime

ç¾åœ¨ã¯`inference_onnx.py`ã‚’ç›´æ¥ä½¿ç”¨ï¼š

```powershell
python inference_onnx.py --images <dir> --output <dir> --directml
```

**TODO:** GUIã«æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠæ©Ÿèƒ½ã‚’è¿½åŠ ï¼ˆv2.1äºˆå®šï¼‰

---

## ğŸ“ æŠ€è¡“è©³ç´°

### ONNXå½¢å¼ã®åˆ©ç‚¹

1. **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯éä¾å­˜:** PyTorchä»¥å¤–ã®ç’°å¢ƒã§ã‚‚å‹•ä½œ
2. **æœ€é©åŒ–:** ã‚°ãƒ©ãƒ•æœ€é©åŒ–ã«ã‚ˆã‚Šé«˜é€ŸåŒ–
3. **GPUäº’æ›æ€§:** DirectML, CUDA, OpenVINOãªã©å¤šæ§˜ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
4. **å±•é–‹æ€§:** ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã‚„ãƒ¢ãƒã‚¤ãƒ«ã§ã‚‚ä½¿ç”¨å¯èƒ½

### DirectMLã¨ã¯

MicrosoftãŒé–‹ç™ºã—ãŸGPUåŠ é€Ÿãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚DirectX 12ãƒ™ãƒ¼ã‚¹ã§å‹•ä½œã—ã€
NVIDIA, AMD, Intelç­‰ã®GPUã§å‹•ä½œã—ã¾ã™ã€‚

**å¯¾å¿œGPU:**
- NVIDIA GeForceï¼ˆå…¨ã‚·ãƒªãƒ¼ã‚ºï¼‰
- AMD Radeon
- Intel Arc/Iris

### ONNX Runtimeã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰

| ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | å¯¾å¿œç’°å¢ƒ | GPU | å‚™è€ƒ |
|------------|---------|-----|------|
| CPUExecutionProvider | All | âŒ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ |
| CUDAExecutionProvider | Linux/Win | âœ… | NVIDIA GPU |
| DmlExecutionProvider | Windows | âœ… | DirectMLï¼ˆæ¨å¥¨ï¼‰ |
| TensorrtExecutionProvider | Linux/Win | âœ… | NVIDIA GPUï¼ˆæœ€é€Ÿï¼‰ |
| OpenVINOExecutionProvider | All | âœ… | Intel GPU/CPU |

---

## ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚’è©¦ã™**
   ```powershell
   python export_onnx.py
   ```

2. **CPUæ¨è«–ã§ãƒ†ã‚¹ãƒˆ**
   ```powershell
   python inference_onnx.py --images test_images --output output_test --device cpu
   ```

3. **GPUæ¨è«–ã‚’è©¦ã™**
   ```powershell
   python inference_onnx.py --images test_images --output output_test --directml
   ```

4. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ**
   - PyTorch vs ONNX Runtime
   - CPU vs GPU

5. **æœ¬ç•ªç’°å¢ƒã§ä½¿ç”¨**
   - å¤§é‡ã®ç”»åƒã§æ¨è«–
   - è¡Œå‹•è§£æã«çµ±åˆ

---

## ğŸ“š å‚è€ƒãƒªãƒ³ã‚¯

- **ONNX Runtime**: https://onnxruntime.ai/
- **DirectML**: https://github.com/microsoft/DirectML
- **ONNX**: https://onnx.ai/

---

**æœ€çµ‚æ›´æ–°: 2026å¹´2æœˆ7æ—¥**
